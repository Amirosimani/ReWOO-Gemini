{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3awxuitJ4D9fK4FH0v9wM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirosimani/ReWOO-Gemini/blob/main/ReWOO_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n"
      ],
      "metadata": {
        "id": "YJPLxIdz2WXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|||\n",
        "|----------|-------------|\n",
        "| Author(s)   | amirimani@ |\n",
        "| Last updated | 5/02/2025 |\n",
        "<br><br>\n"
      ],
      "metadata": {
        "id": "boZaMQp4J2kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet datasets\n",
        "!pip install --quiet langchain langchain_community\n",
        "!pip install --quiet langchain_google_genai langchain_google_community\n",
        "!pip install --quiet tiktoken"
      ],
      "metadata": {
        "id": "_In1NIeyqmD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from typing import List, Dict, Union, Callable, Any\n",
        "\n",
        "from langchain import hub\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.schema import AgentAction, AgentFinish, LLMResult, BaseMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.tools import Tool\n",
        "\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from google.genai.types import GenerateContentConfig"
      ],
      "metadata": {
        "id": "K9cZba0Jv34Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "MODEL = \"gemini-2.0-flash\"\n",
        "MAX_ITERATIONS = 8\n",
        "MAX_EXECUTION_TIME = 60\n",
        "\n",
        "GENERATION_CONFIG = {\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 20,\n",
        "    \"candidate_count\": 1,\n",
        "    \"max_output_tokens\": 8192,\n",
        "}\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE-API')\n",
        "CSE_ID = userdata.get(\"CSE-ID\")"
      ],
      "metadata": {
        "id": "MRqm03Sz5_fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "[**StrategyQA**](https://paperswithcode.com/dataset/strategyqa) is a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. It includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "pQxf0kNGBtVN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfevMoHIBgbP"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"ChilleD/StrategyQA\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the training split\n",
        "train_ds = ds[\"train\"]\n",
        "test_ds = ds['test']"
      ],
      "metadata": {
        "id": "3nXOp5QCCMxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM"
      ],
      "metadata": {
        "id": "2ceIKSwrsu2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "\n",
        "Use Gemnini with no tools and not fancy prompts!"
      ],
      "metadata": {
        "id": "dpldCVg60L9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "daCJHjFf5rAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, model=MODEL, config=GENERATION_CONFIG):\n",
        "\n",
        "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "    config[\"system_instruction\"] = \"Once you are done finding the answer, only return Yes or No\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=prompt,\n",
        "        config=GenerateContentConfig(**config)\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    wall_time = end_time - start_time\n",
        "\n",
        "    result = {\n",
        "        'input': prompt,\n",
        "        'output': response.text.strip(),\n",
        "        'total_token': response.usage_metadata.total_token_count,\n",
        "        'wall_time': wall_time\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "aJKvz8tz0tql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate(train_ds[1]['question'])"
      ],
      "metadata": {
        "id": "iwrw4FmJnH4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReAct Agent"
      ],
      "metadata": {
        "id": "JkMOsPAp0KMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReActAgentExecutor:\n",
        "    \"\"\"\n",
        "    A class to run the ReAct agent with specified configurations and tools.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = MODEL,\n",
        "        generation_config: Dict = GENERATION_CONFIG,\n",
        "        max_iterations: int = MAX_ITERATIONS,\n",
        "        max_execution_time: int = MAX_EXECUTION_TIME,\n",
        "        google_api_key: str = GOOGLE_API_KEY,\n",
        "        cse_id: str = CSE_ID,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.generation_config = generation_config\n",
        "        # self.safety_settings = safety_settings\n",
        "        self.max_iterations = max_iterations\n",
        "        self.max_execution_time = max_execution_time\n",
        "        self.google_api_key = google_api_key\n",
        "        self.cse_id = cse_id\n",
        "        self.llm = None\n",
        "        self.tools = None\n",
        "        self.agent = None\n",
        "        self.agent_executor = None\n",
        "        self.token_callback = None\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_tools()\n",
        "        self._setup_agent()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "        \"\"\"Initializes the language model.\"\"\"\n",
        "        if not GEMINI_API_KEY or GEMINI_API_KEY == \"your_gemini_api_key\":\n",
        "            raise ValueError(\"GEMINI_API_KEY must be set to a valid API key.\")\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=self.model,\n",
        "            google_api_key=GEMINI_API_KEY,\n",
        "            generation_config=self.generation_config,\n",
        "        )\n",
        "\n",
        "    def _setup_tools(self):\n",
        "        \"\"\"Sets up the tools for the agent.\"\"\"\n",
        "        search = GoogleSearchAPIWrapper(\n",
        "            google_api_key=self.google_api_key, google_cse_id=self.cse_id\n",
        "        )\n",
        "\n",
        "        self.tools = [\n",
        "            Tool(\n",
        "                name=\"Google Search\",\n",
        "                func=search.run,\n",
        "                description=\"Useful for finding information on current events, comparisons, or diverse perspectives.\",\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Sets up the ReAct agent and executor.\"\"\"\n",
        "        prompt = hub.pull(\"hwchase17/react\")\n",
        "        system_instruction = \"Once you are done finding the answer, only return Yes or No\"\n",
        "        prompt.template = system_instruction + \"\\n\" + prompt.template\n",
        "\n",
        "        self.agent = create_react_agent(self.llm, self.tools, prompt)\n",
        "\n",
        "        self.token_callback = TokenCountingCallbackHandler(self.model)\n",
        "        self.agent_executor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=self.tools,\n",
        "            verbose=False,\n",
        "            handle_parsing_errors=True,\n",
        "            max_iterations=self.max_iterations,\n",
        "            max_execution_time=self.max_execution_time,\n",
        "            callbacks=[self.token_callback],\n",
        "        )\n",
        "\n",
        "    def run(self, input_data: Union[Dict, str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Runs the agent with the given input data.\n",
        "\n",
        "        Args:\n",
        "            input_data: Either a dictionary or a string representing the input for the agent.\n",
        "\n",
        "        Returns:\n",
        "            The output from the agent.\n",
        "        \"\"\"\n",
        "        if isinstance(input_data, str):\n",
        "            input_data = {\"input\": input_data}\n",
        "\n",
        "        start_time = time.time()  # Start timing\n",
        "        try:\n",
        "            result = self.agent_executor.invoke(input_data)\n",
        "            result[\"total_token\"] = self.token_callback.total_token\n",
        "\n",
        "            self.token_callback.reset()  # Reset after each run\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            result = {\"error\": str(e)}\n",
        "        end_time = time.time()  # End timing\n",
        "\n",
        "        # Log wall time\n",
        "        wall_time = end_time - start_time\n",
        "        print(f\"Wall time for execution: {wall_time:.2f} seconds\")\n",
        "        result[\"wall_time\"] = wall_time\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class TokenCountingCallbackHandler(BaseCallbackHandler):\n",
        "    \"\"\"Callback handler for counting tokens used by the language model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        self.total_token = 0\n",
        "        self.prompt_tokens = 0\n",
        "        self.completion_tokens = 0\n",
        "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    def on_llm_start(\n",
        "        self, serialized: Dict[str, any], prompts: List[str], **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"Collect prompt tokens when LLM starts.\"\"\"\n",
        "        for prompt in prompts:\n",
        "            self.prompt_tokens += len(self.encoding.encode(prompt))\n",
        "\n",
        "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
        "        \"\"\"Collect completion tokens when LLM finishes generating.\"\"\"\n",
        "        if response.generations:\n",
        "            for generation_list in response.generations:\n",
        "                for generation in generation_list:\n",
        "                    if generation.text:\n",
        "                        self.completion_tokens += len(\n",
        "                            self.encoding.encode(generation.text)\n",
        "                        )\n",
        "\n",
        "    def on_agent_action(self, action: AgentAction, **kwargs) -> None:\n",
        "        \"\"\"Increment token count on agent action.\"\"\"\n",
        "        if action.log:\n",
        "            self.total_token += len(self.encoding.encode(action.log))\n",
        "\n",
        "    def on_agent_finish(self, finish: AgentFinish, **kwargs) -> None:\n",
        "        \"\"\"Increment token count on agent finish.\"\"\"\n",
        "        if finish.log:\n",
        "            self.total_token += len(self.encoding.encode(finish.log))\n",
        "\n",
        "    def on_chain_end(self, outputs, **kwargs) -> None:\n",
        "        \"\"\"Print the total tokens used when the chain finishes.\"\"\"\n",
        "        self.total_token += self.completion_tokens + self.prompt_tokens\n",
        "        print(f\"Prompt tokens: {self.prompt_tokens}\")\n",
        "        print(f\"Completion tokens: {self.completion_tokens}\")\n",
        "        print(f\"Total tokens used in this chain: {self.total_token}\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the counters for the next chain run.\"\"\"\n",
        "        self.total_token = 0\n",
        "        self.prompt_tokens = 0\n",
        "        self.completion_tokens = 0\n"
      ],
      "metadata": {
        "id": "XhDsoEW8Hfxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agent_executor = ReActAgentExecutor()\n",
        "# result = agent_executor.run(train_ds[1][\"question\"])"
      ],
      "metadata": {
        "id": "dNmyUg2nH8nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReWOO"
      ],
      "metadata": {
        "id": "aM_tfetUo5tr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReWOO: Decoupling Reasoning from Observations\n",
        "for Efficient Augmented Language Models [paper](https://arxiv.org/pdf/2305.18323)\n",
        "\n",
        "based on the implementation [here](https://github.com/billxbf/ReWOO/tree/main)"
      ],
      "metadata": {
        "id": "MwhdGb0u0e4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Simplified Worker ---\n",
        "class GoogleSearchWorker:\n",
        "    def __init__(self, name=\"Google\"):\n",
        "        self.name = name\n",
        "        self.google_api_key = GOOGLE_API_KEY\n",
        "        self.cse_id = CSE_ID\n",
        "        self.description = \"Worker that searches results from Google. Useful when you need to find short \" \\\n",
        "                           \"and succinct answers about a specific topic. Input should be a search query.\"\n",
        "\n",
        "    def run(self, input):\n",
        "        search = GoogleSearchAPIWrapper(\n",
        "            google_api_key=self.google_api_key, google_cse_id=self.cse_id\n",
        "        )\n",
        "        # Get the results from the API\n",
        "        results = search.results(input, 1)\n",
        "\n",
        "        # Print the structure of the results for debugging\n",
        "        print(\"Results Structure:\", results)\n",
        "\n",
        "        evidence = \"\"\n",
        "        for result in results:\n",
        "            # Check if 'snippet' exists, otherwise use 'title' or 'body'\n",
        "            if \"snippet\" in result:\n",
        "                evidence += result[\"snippet\"]\n",
        "            elif \"title\" in result:\n",
        "                evidence += result[\"title\"]\n",
        "            elif \"body\" in result:  # Use \"body\" as a fallback\n",
        "                evidence += result[\"body\"]\n",
        "            else:\n",
        "                print(\"Warning: No relevant information found in result:\", result)\n",
        "\n",
        "        return evidence\n",
        "\n",
        "# --- LLM Node (Simplified) ---\n",
        "class LLMNode:\n",
        "    def __init__(self, name, model_name, stop=None, input_type=str, output_type=str):\n",
        "        self.name = name\n",
        "        self.model_name = model_name\n",
        "        self.model = model_name\n",
        "        self.stop = stop\n",
        "        self.input_type = input_type\n",
        "        self.output_type = output_type\n",
        "        self.generation_config = GENERATION_CONFIG\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=self.model,\n",
        "            google_api_key=GEMINI_API_KEY,\n",
        "            generation_config=self.generation_config,\n",
        "            # safety_settings=self.safety_settings,\n",
        "        )\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    def call_llm(self, prompt, stop):\n",
        "        if isinstance(prompt, list):\n",
        "            prompt_template = PromptTemplate(template=prompt[0], input_variables=[\"question\"])\n",
        "            prompt_text = prompt[0]\n",
        "            llm_chain = LLMChain(prompt=prompt_template, llm=self.llm, verbose=False)\n",
        "            response = llm_chain(prompt[1])\n",
        "            output = response[\"text\"].strip()\n",
        "        else:\n",
        "            prompt_template = PromptTemplate(template=prompt, input_variables=[])\n",
        "            prompt_text = prompt\n",
        "            llm_chain = LLMChain(prompt=prompt_template, llm=self.llm, verbose=False)\n",
        "            response = llm_chain({})\n",
        "            output = response[\"text\"].strip()\n",
        "\n",
        "        prompt_tokens = len(self.tokenizer.encode(prompt_text))\n",
        "        completion_tokens = len(self.tokenizer.encode(output))\n",
        "        return {\n",
        "            \"output\": output,\n",
        "            \"prompt_tokens\": prompt_tokens,\n",
        "            \"completion_tokens\": completion_tokens\n",
        "        }\n",
        "\n",
        "# --- Planner ---\n",
        "class Planner(LLMNode):\n",
        "    def __init__(self, model_name=\"gemini-pro\", fewshot=\"\"):\n",
        "        super().__init__(\"Planner\", model_name, stop=None, input_type=str, output_type=str)\n",
        "        self.worker_prompt = \"Tools can be one of the following:\\nGoogle[input]: Worker that searches results from Google. Useful when you need to find short and succinct answers about a specific topic. Input should be a search query.\\n\\n\"\n",
        "        self.prefix = \"For the following tasks, make plans that can solve the problem step-by-step. For each plan, \" \\\n",
        "                     \"indicate which external tool together with tool input to retrieve evidence. You can store the \" \\\n",
        "                     \"evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...) \\n\\n\"\n",
        "        self.suffix = \"Begin! Describe your plans with rich details. Each Plan should be followed by only one #E.\\n\\n\"\n",
        "        self.fewshot = fewshot\n",
        "\n",
        "    def run(self, input, log=False):\n",
        "        prompt = self.prefix + self.worker_prompt + self.fewshot + self.suffix + input + '\\n'\n",
        "        response = self.call_llm(prompt, self.stop)\n",
        "        if log:\n",
        "            return response\n",
        "        return response[\"output\"]\n",
        "\n",
        "# --- Solver ---\n",
        "class Solver(LLMNode):\n",
        "    def __init__(self, model_name=\"gemini-pro\"):\n",
        "        super().__init__(\"Solver\", model_name, stop=None, input_type=str, output_type=str)\n",
        "        self.prefix = \"Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\\n\\n\"\n",
        "        self.suffix = \"\\nNow begin to solve the task or problem. Respond with the answer directly with no extra words. Your answer should be either Yes or No\\n\\n\"\n",
        "\n",
        "    def run(self, input, worker_log, log=False):\n",
        "        prompt = self.prefix + input + \"\\n\" + worker_log + self.suffix + input + '\\n'\n",
        "        response = self.call_llm(prompt, self.stop)\n",
        "        if log:\n",
        "            return response\n",
        "        return response[\"output\"]\n",
        "\n",
        "# --- Main PWS Class ---\n",
        "class PWS:\n",
        "    def __init__(self, planner_model=\"gemini-pro\", solver_model=\"gemini-pro\", fewshot=\"\"):\n",
        "        self.worker = GoogleSearchWorker()\n",
        "        self.planner = Planner(model_name=planner_model, fewshot=fewshot)\n",
        "        self.solver = Solver(model_name=solver_model)\n",
        "        self.plans = []\n",
        "        self.planner_evidences = {}\n",
        "        self.worker_evidences = {}\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    def run(self, input):\n",
        "        self._reinitialize()\n",
        "        result = {}\n",
        "        st = time.time()\n",
        "\n",
        "        # Plan\n",
        "        planner_response = self.planner.run(input, log=True)\n",
        "        plan = planner_response[\"output\"]\n",
        "\n",
        "        # Store the planner input before calling call_llm\n",
        "        planner_input = self.planner.prefix + self.planner.worker_prompt + self.planner.fewshot + self.planner.suffix + input + '\\n'\n",
        "\n",
        "        planner_log = planner_input + planner_response[\"output\"]\n",
        "\n",
        "        self.plans = self._parse_plans(plan)\n",
        "        self.planner_evidences = self._parse_planner_evidences(plan)\n",
        "\n",
        "        # --- Validation and Error Handling ---\n",
        "        valid_plan = self._validate_plan()\n",
        "        if not valid_plan:\n",
        "            print(\"Warning: Invalid plan generated. Skipping worker and passing the question to solver.\")\n",
        "            worker_log = \"\"\n",
        "            solver_input = self.solver.prefix + input + \"\\n\" + self.solver.suffix + input + '\\n'\n",
        "        else:\n",
        "            # Work\n",
        "            self._get_worker_evidences()\n",
        "            worker_log = \"\"\n",
        "            total_worker_tokens = 0\n",
        "            for i in range(len(self.plans)):\n",
        "                e = f\"#E{i + 1}\"\n",
        "                if e in self.worker_evidences:\n",
        "                    worker_log += f\"{self.plans[i]}\\nEvidence:\\n{self.worker_evidences[e]}\\n\"\n",
        "                    total_worker_tokens += self._count_tokens(self.worker_evidences[e])\n",
        "                else:\n",
        "                    worker_log += f\"{self.plans[i]}\\nEvidence:\\nNo evidence found for {e}\\n\"\n",
        "                    print(f\"Warning: No evidence found for {e} in self.worker_evidences\")\n",
        "\n",
        "        # Solve\n",
        "        solver_response = self.solver.run(input, worker_log, log=True)\n",
        "\n",
        "        # Similar fix for solver_log\n",
        "        if valid_plan:\n",
        "            solver_input = self.solver.prefix + input + \"\\n\" + worker_log + self.solver.suffix + input + '\\n'\n",
        "        else:\n",
        "            solver_input = self.solver.prefix + input + \"\\n\" + self.solver.suffix + input + '\\n'\n",
        "\n",
        "        output = solver_response[\"output\"]\n",
        "        solver_log = solver_input + solver_response[\"output\"]\n",
        "\n",
        "        result[\"wall_time\"] = time.time() - st\n",
        "        result[\"input\"] = input\n",
        "        result[\"output\"] = output\n",
        "        result[\"planner_log\"] = planner_log\n",
        "        result[\"worker_log\"] = worker_log\n",
        "        result[\"solver_log\"] = solver_log\n",
        "        result[\"steps\"] = len(self.plans) + 1\n",
        "        result[\"prompt_tokens\"] = planner_response[\"prompt_tokens\"] + solver_response[\"prompt_tokens\"]\n",
        "        result[\"completion_tokens\"] = planner_response[\"completion_tokens\"] + solver_response[\"completion_tokens\"]\n",
        "\n",
        "        if valid_plan:\n",
        "            result[\"total_token\"] = result[\"completion_tokens\"] + total_worker_tokens\n",
        "        else:\n",
        "            result[\"total_token\"] = result[\"completion_tokens\"]\n",
        "\n",
        "\n",
        "        result = {k: result[k] for k in [\"input\", \"output\", \"wall_time\", \"total_token\"]}\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _validate_plan(self):\n",
        "        \"\"\"\n",
        "        Validates if the generated plan has the correct #E notation in sequence.\n",
        "        \"\"\"\n",
        "        for i in range(len(self.plans)):\n",
        "            expected_evidence_key = f\"#E{i + 1}\"\n",
        "            if expected_evidence_key not in self.planner_evidences:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def _parse_plans(self, response):\n",
        "        plans = []\n",
        "        for line in response.splitlines():\n",
        "            if line.startswith(\"Plan:\"):\n",
        "                plans.append(line)\n",
        "        return plans\n",
        "\n",
        "    def _parse_planner_evidences(self, response):\n",
        "        evidences = {}\n",
        "        for line in response.splitlines():\n",
        "            if line.startswith(\"#\") and line[1] == \"E\" and line[2].isdigit():\n",
        "                parts = line.split(\"=\", 1)  # Split into at most 2 parts\n",
        "                if len(parts) == 2:\n",
        "                    e, tool_call = parts\n",
        "                    e, tool_call = e.strip(), tool_call.strip()\n",
        "                    evidences[e] = tool_call\n",
        "                else:\n",
        "                    # Handle cases where there's no '=' after #E\n",
        "                    e = parts[0].strip()\n",
        "                    evidences[e] = \"No evidence found\"\n",
        "                    print(f\"Warning: Invalid planner evidence format: {line}\")\n",
        "        return evidences\n",
        "\n",
        "    def _get_worker_evidences(self):\n",
        "        for e, tool_call in self.planner_evidences.items():\n",
        "            if not tool_call.startswith(\"Google[\"):\n",
        "                self.worker_evidences[e] = \"No evidence found\"\n",
        "                continue\n",
        "            tool_input = tool_call[7:-1]\n",
        "            for var in re.findall(r\"#E\\d+\", tool_input):\n",
        "                if var in self.worker_evidences:\n",
        "                    tool_input = tool_input.replace(var, \"[\" + self.worker_evidences[var] + \"]\")\n",
        "\n",
        "            self.worker_evidences[e] = self.worker.run(tool_input)\n",
        "\n",
        "    def _reinitialize(self):\n",
        "        self.plans = []\n",
        "        self.planner_evidences = {}\n",
        "        self.worker_evidences = {}\n",
        "\n",
        "    def _count_tokens(self, text):\n",
        "        return len(self.tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "MMDEkPJrulcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rewoo_executor = PWS()\n",
        "# result = rewoo_executor.run(train_ds[1][\"question\"])\n",
        "# result"
      ],
      "metadata": {
        "id": "hfK93ZR2qSBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run evalaution"
      ],
      "metadata": {
        "id": "PZjBg0aDvgrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(data: List[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n",
        "    \"\"\"Processes data and runs three functions, storing results in a dictionary.\"\"\"\n",
        "\n",
        "    results = {}\n",
        "    question = data[\"question\"]\n",
        "\n",
        "    generate_result = generate(question)\n",
        "    agent_result = ReActAgentExecutor().run(question)\n",
        "    rewoo_result = PWS().run(question)\n",
        "\n",
        "    return {\n",
        "        \"gemini_2_flash\": generate_result,\n",
        "        \"gemini_2_lite\": generate(prompt=\"question\",\n",
        "                                  model=\"gemini-2.0-flash-lite-preview-02-05\"),\n",
        "        \"agent_executor\": agent_result,\n",
        "        \"rewoo_executor\": rewoo_result,\n",
        "        \"gt\": data['answer']\n",
        "    }"
      ],
      "metadata": {
        "id": "6Incv-aTvfwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('./results_train_20250205__2.jsonl', \"a\") as file:\n",
        "    for data in train_ds.select(range(150, len(train_ds))):\n",
        "        result = run_experiment(data)\n",
        "        json.dump(result, file)\n",
        "        file.write(\"\\n\")"
      ],
      "metadata": {
        "id": "f6S_dje1xadW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyse results"
      ],
      "metadata": {
        "id": "y7olBcKepaN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import statistics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
      ],
      "metadata": {
        "id": "fr0ZHCL3pYRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = \"./\"  # Change this to your directory containing JSONL files\n",
        "all_data = []\n",
        "\n",
        "jsonl_files = glob.glob(os.path.join(directory_path, \"*.jsonl\"))\n",
        "\n",
        "for file_path in jsonl_files:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            try:\n",
        "                all_data.append(json.loads(line.strip()))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Skipping invalid line in {file_path}: {e}\")\n",
        "\n",
        "print(f\"Loaded {len(all_data)} records from {len(jsonl_files)} files.\")"
      ],
      "metadata": {
        "id": "bz4OSUN2pmph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(all_data)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "A8THrafSNZwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def expand_dict_columns(df):\n",
        "\n",
        "    new_df = df.copy()  # Create a copy to avoid modifying the original DataFrame\n",
        "\n",
        "    cols_to_expand = []\n",
        "    for col in new_df.columns:\n",
        "        if isinstance(new_df[col].iloc[0], dict):  # Check if the first element is a dict\n",
        "            cols_to_expand.append(col)\n",
        "\n",
        "    if not cols_to_expand:\n",
        "        return new_df  # Return original if no dict columns\n",
        "\n",
        "    for col in cols_to_expand:\n",
        "        try:\n",
        "            # Efficiently handle potentially mixed data types by converting to string\n",
        "            expanded_data = pd.json_normalize(new_df[col].astype(str).apply(eval))  # eval is generally unsafe, but we are converting to string first.\n",
        "            expanded_data = expanded_data.add_prefix(col + \"_\")\n",
        "            new_df = new_df.drop(columns=[col])  # Drop the original dictionary column\n",
        "            new_df = pd.concat([new_df, expanded_data], axis=1)\n",
        "\n",
        "        except (TypeError, ValueError, SyntaxError) as e:\n",
        "            print(f\"Warning: Could not expand column '{col}'.  Likely inconsistent data types.  Skipping. Error: {e}\")\n",
        "            # Handle exceptions gracefully, skip the column, and continue.\n",
        "            continue\n",
        "\n",
        "\n",
        "    return new_df"
      ],
      "metadata": {
        "id": "AjSSNxze5u4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = expand_dict_columns(df)\n",
        "df = df.dropna(subset=['agent_executor_output', 'agent_executor_output'])\n",
        "df.shape"
      ],
      "metadata": {
        "id": "uE3Hmoph5zFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n",
        "\n"
      ],
      "metadata": {
        "id": "Kf73Vwzj5pcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_cols = [col for col in df.columns if \"output\" in col]\n",
        "output_cols"
      ],
      "metadata": {
        "id": "qmh3kkuUOP7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in output_cols:\n",
        "    df[col] = df[col].apply(lambda x: str(x).lower() if isinstance(x, str) else x)  # Conditional lowercasing\n",
        "    df[col] = df[col].replace({'yes': True, 'no': False})\n",
        "    df[col] = df[col].astype(str)"
      ],
      "metadata": {
        "id": "_ULfYX7T1O38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in output_cols:\n",
        "    unique_count = df[col].nunique()\n",
        "    unique_values = df[col].unique()\n",
        "    print(f\"Column '{col}' has {unique_count} unique values.\", unique_values)"
      ],
      "metadata": {
        "id": "m2b8xbEQOdHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in output_cols:\n",
        "    unique_count = df[col].value_counts()\n",
        "    print(f\"Column '{col}' has {unique_count}\")"
      ],
      "metadata": {
        "id": "miZNbqn8Onwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['gt'] = df['gt'].astype(str)"
      ],
      "metadata": {
        "id": "MKClGhNCRitb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report(executor, data):\n",
        "  y_pred = [p if p in [\"True\", \"False\"] else \"Wrong\" for p in data[executor]]\n",
        "  y_true = [t if t in [\"True\", \"False\"] else \"Wrong\" for t in data['gt']]\n",
        "\n",
        "  return classification_report(y_true, y_pred, labels=[\"True\", \"False\", \"Wrong\"],\n",
        "                               output_dict=True)"
      ],
      "metadata": {
        "id": "e_y6wFaGRBGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reports = {}\n",
        "for col in output_cols:\n",
        "\n",
        "  reports[col] = generate_report(col, df)\n",
        "  # print(f\"result for {col} is {r}\")"
      ],
      "metadata": {
        "id": "KzOI95FZR3Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sel_cols = [col for col in df.columns if \"token\" in col]\n",
        "# sel_cols = [col for col in df.columns if \"time\" in col]\n",
        "\n",
        "df[sel_cols].describe()\n",
        "\n",
        "\n",
        "mean_values = df[sel_cols].mean()\n",
        "std_values = df[sel_cols].std()\n",
        "\n",
        "# Create a summary table\n",
        "summary = pd.DataFrame({'Mean': mean_values, 'Std Dev': std_values})\n",
        "summary = summary.round(2)\n",
        "\n",
        "# Display the summary table\n",
        "print(summary.to_markdown(numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "id": "ovgHd-xTyhs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reports = pd.DataFrame({model: {metric: report[\"weighted avg\"][metric] for metric in [\"precision\", \"recall\", \"f1-score\"]}\n",
        "                           for model, report in reports.items()}).T\n",
        "\n",
        "df_reports"
      ],
      "metadata": {
        "id": "tR52vt0eTFZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reports['mean_wall_time'] = list(mean_values)\n",
        "df_reports"
      ],
      "metadata": {
        "id": "E5cw0jFS7RtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reports['mean_token__count'] = list(mean_values)\n",
        "df_reports"
      ],
      "metadata": {
        "id": "xmN20rj_7m2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from math import pi\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def create_spider_chart(df, title=\"Spider Chart\"):\n",
        "\n",
        "    categories = list(df.columns)  # The columns become the categories\n",
        "    num_categories = len(categories)\n",
        "\n",
        "    # We need to \"close\" the plot by repeating the first value at the end\n",
        "    angles = [n / float(num_categories) * 2 * pi for n in range(num_categories)]\n",
        "    angles += angles[:1] # Close the plot\n",
        "\n",
        "    # Color mapping based on index names\n",
        "    unique_indices = df.index.unique()\n",
        "    color_map = plt.cm.get_cmap(\"viridis\", len(unique_indices))  # Or any other colormap\n",
        "\n",
        "    plt.figure(figsize=(8, 8))  # Adjust figure size as needed\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "\n",
        "    for i, index_name in enumerate(unique_indices):\n",
        "        values = df.loc[index_name].values.flatten().tolist()  # Get values for current index\n",
        "        values += values[:1]  # Close the plot\n",
        "\n",
        "        color = color_map(i)  # Get color from colormap\n",
        "        ax.plot(angles, values, marker='o', linestyle='-', color=color, label=index_name)\n",
        "        ax.fill(angles, values, alpha=0.25, color=color)  # Fill area under the line\n",
        "\n",
        "    ax.set_xticks(angles[:-1])  # Set ticks for each category\n",
        "    ax.set_xticklabels(categories)  # Set category labels\n",
        "    ax.set_yticklabels([]) # Remove y ticks\n",
        "\n",
        "    ax.grid(True)\n",
        "    plt.title(title, y=1.1)  # Adjust title position\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1)) # Place legend outside the plot\n",
        "\n",
        "    plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "    plt.show()\n",
        "\n",
        "def create_standardized_spider_chart(df, title=\"Standardized Spider Chart\"):\n",
        "    \"\"\"Creates a spider chart with standardization (Z-score).\"\"\"\n",
        "\n",
        "    df_scaled = df.copy()\n",
        "    for col in df_scaled.columns:\n",
        "        scaler = StandardScaler()\n",
        "        df_scaled[col] = scaler.fit_transform(df_scaled[[col]])\n",
        "\n",
        "    create_spider_chart(df_scaled, title=title)"
      ],
      "metadata": {
        "id": "B32FzQL675h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "create_standardized_spider_chart(df_reports)"
      ],
      "metadata": {
        "id": "6dFhBCWW78xU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sel = df.filter(like=\"token\")\n",
        "\n",
        "# Calculate the average value for each column\n",
        "column_means = df_sel.mean()\n",
        "\n",
        "# Sort the columns based on their average values (ascending)\n",
        "sorted_columns = column_means.sort_values().index\n",
        "\n",
        "# Melt the DataFrame using the sorted column order\n",
        "df_melted = pd.melt(df_sel[sorted_columns], var_name='column', value_name='value')\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create violin plot, ensuring order follows sorted means\n",
        "sns.violinplot(x='value', y='column', data=df_melted, order=sorted_columns, orient='h', palette='Set3', linewidth=1.2, cut=0, scale=\"width\")\n",
        "\n",
        "# Enhancements for better visualization\n",
        "plt.xlabel('Total Token Count', fontsize=12)\n",
        "plt.ylabel('Executor', fontsize=12)\n",
        "plt.title('Token Count Violin Plots', fontsize=14, fontweight='bold')\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "sns.despine(left=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HX3rzc-Xyv-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sel = df.filter(like=\"wall\")\n",
        "\n",
        "# Calculate the average value for each column\n",
        "column_means = df_sel.mean()\n",
        "\n",
        "# Sort the columns based on their average values (ascending)\n",
        "sorted_columns = column_means.sort_values().index\n",
        "\n",
        "# Melt the DataFrame using the sorted column order\n",
        "df_melted = pd.melt(df_sel[sorted_columns], var_name='column', value_name='value')\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create violin plot, ensuring order follows sorted means\n",
        "sns.violinplot(x='value', y='column', data=df_melted, order=sorted_columns, orient='h', palette='Set3', linewidth=1.2, cut=0, scale=\"width\")\n",
        "\n",
        "# Enhancements for better visualization\n",
        "plt.xlabel('Total Wall Time', fontsize=12)\n",
        "plt.ylabel('Executor', fontsize=12)\n",
        "plt.title('Latency Plot', fontsize=14, fontweight='bold')\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "sns.despine(left=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7fsfIhzby-zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sel.describe()"
      ],
      "metadata": {
        "id": "HTcVT5auUV1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "\n",
        "[x] add baseline\n",
        "\n",
        "[] native google search retrieval\n",
        "\n",
        "[x] callback/count tokens for react\n",
        "\n",
        "[x] ReWOO\n",
        "\n",
        "[x] add walltime to all other functions -> # token, time, accuracy\n",
        "\n",
        "[] spider charts\n",
        "\n",
        "[x] deepseek"
      ],
      "metadata": {
        "id": "6AJANtdgltx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy DeepSeek"
      ],
      "metadata": {
        "id": "Ms8iVZ77NwZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install --upgrade --user --quiet \"google-cloud-aiplatform[reasoningengine, evaluation]\" \"openai\" \"smolagents\" \\\n",
        "#     \"cloudpickle==3.0.0\" \\\n",
        "#     \"pydantic>=2.10\" \\\n",
        "#     \"requests\""
      ],
      "metadata": {
        "id": "fNR-ttnzyPeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "a9V4DsmWagjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "\n",
        "# if \"google.colab\" in sys.modules:\n",
        "#     from google.colab import auth\n",
        "\n",
        "#     auth.authenticate_user()"
      ],
      "metadata": {
        "id": "F_kjqksQZ4iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PROJECT_ID = \"amir-genai-bb\"\n",
        "# LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "# vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
      ],
      "metadata": {
        "id": "3cMraCEmZ9IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUCKET_NAME = \"deepseek-amir\"\n",
        "# BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "# ! gsutil mb -p $PROJECT_ID -l $LOCATION $BUCKET_URI\n",
        "# MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\""
      ],
      "metadata": {
        "id": "ThQ9iXvVNX4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# import vertexai\n",
        "# from huggingface_hub import get_token\n",
        "# from google.cloud import aiplatform"
      ],
      "metadata": {
        "id": "-LTjrM9raMPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deepseek_model = aiplatform.Model.upload(\n",
        "#     display_name=MODEL_ID.replace(\"/\", \"--\").lower(),\n",
        "#     serving_container_image_uri=\"us-docker.pkg.dev/deeplearning-platform-release/vertex-model-garden/vllm-inference.cu121.0-6.ubuntu2204.py310\",\n",
        "#     serving_container_args=[\n",
        "#         \"python\",\n",
        "#         \"-m\",\n",
        "#         \"vllm.entrypoints.api_server\",\n",
        "#         \"--host=0.0.0.0\",\n",
        "#         \"--port=8080\",\n",
        "#         f\"--model={MODEL_ID}\",\n",
        "#         \"--tensor-parallel-size=1\",\n",
        "#         \"--max-model-len=16384\",\n",
        "#         \"--enforce-eager\",\n",
        "#     ],\n",
        "#     serving_container_ports=[8080],\n",
        "#     serving_container_predict_route=\"/generate\",\n",
        "#     serving_container_health_route=\"/ping\",\n",
        "#     serving_container_environment_variables={\n",
        "#         \"HF_TOKEN\": get_token(),\n",
        "#         \"DEPLOY_SOURCE\": \"notebook\",\n",
        "#     },\n",
        "# )\n",
        "# deepseek_model.wait()"
      ],
      "metadata": {
        "id": "5BiO3d0MaxFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deepseek_endpoint = aiplatform.Endpoint.create(\n",
        "#     display_name=MODEL_ID.replace(\"/\", \"--\").lower() + \"-endpoint\"\n",
        "# )\n",
        "\n",
        "# deployed_deepseek_model = deepseek_model.deploy(\n",
        "#     endpoint=deepseek_endpoint,\n",
        "#     machine_type=\"g2-standard-12\",\n",
        "#     accelerator_type=\"NVIDIA_L4\",\n",
        "#     accelerator_count=1,\n",
        "#     sync=False,\n",
        "# )"
      ],
      "metadata": {
        "id": "r4Ty1haXa59H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use DeepSeek"
      ],
      "metadata": {
        "id": "A0fLD_KiN2Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "PROJECT_ID = \"amir-genai-bb\"\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "# Initialize the Vertex AI client\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n"
      ],
      "metadata": {
        "id": "LbPjj0IkSzGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of endpoints\n",
        "endpoints = aiplatform.Endpoint.list()\n",
        "endpoints"
      ],
      "metadata": {
        "id": "bNIQQLY0Nr3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoints[0].name"
      ],
      "metadata": {
        "id": "8rH0WsmRLWzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = aiplatform.Endpoint(endpoint_name=\"5120549895366770688\")"
      ],
      "metadata": {
        "id": "b-iKR1e4S4as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_request = {\n",
        "    \"instances\": [\n",
        "        {\n",
        "            \"@requestFormat\": \"textGeneration\",\n",
        "            \"prompt\":\"Count the number of 'r' in the word Strawberry. only return the final answer.\",\n",
        "            \"max_tokens\": 2048,\n",
        "            \"temperature\": 0.7,\n",
        "        }\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "TUS62Lx_OND9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output.predictions[0]"
      ],
      "metadata": {
        "id": "nPY8-Dw_QqLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output = endpoint.predict(instances=prediction_request[\"instances\"])\n",
        "for prediction in output.predictions[0]:\n",
        "    print(\"------- DeepSeek prediction -------\")\n",
        "    print(prediction[\"message\"][\"content\"])\n",
        "    print(\"---------------------------------\\n\")"
      ],
      "metadata": {
        "id": "PFdisl7RN-Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = endpoint.predict(instances=prediction_request[\"instances\"])"
      ],
      "metadata": {
        "id": "9jNxO2bzUyW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "id": "-FOCCR3FLfpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YCWo6iooVFlN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}