{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUaxvSOqJyb1bE6Q0+TT31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirosimani/ReWOO-Gemini/blob/main/ReWOO_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n"
      ],
      "metadata": {
        "id": "YJPLxIdz2WXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|||\n",
        "|----------|-------------|\n",
        "| Author(s)   | amirimani@ |\n",
        "| Last updated | 24/01/2025 |\n",
        "<br><br>\n"
      ],
      "metadata": {
        "id": "boZaMQp4J2kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet datasets\n",
        "# !pip install --quiet langchain langchain_community\n",
        "# !pip install --quiet langchain_google_genai langchain_google_community\n",
        "# !pip install --quiet wikipedia\n",
        "# !pip install --quiet tiktoken"
      ],
      "metadata": {
        "id": "_In1NIeyqmD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "9a055BpR5zkn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "MODEL = \"gemini-1.5-flash\"\n",
        "MAX_ITERATIONS = 10\n",
        "MAX_EXECUTION_TIME = 90\n",
        "\n",
        "GENERATION_CONFIG = {\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 20,\n",
        "    \"candidate_count\": 1,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"stop_sequences\": [\"STOP!\"],\n",
        "}\n",
        "\n",
        "# SAFETY_SETTINGS = {\n",
        "#     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "#     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "#     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "#     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "# }\n",
        "\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE-API')\n",
        "CSE_ID = userdata.get(\"CSE-ID\")"
      ],
      "metadata": {
        "id": "MRqm03Sz5_fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "[**StrategyQA**](https://paperswithcode.com/dataset/strategyqa) is a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. It includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "pQxf0kNGBtVN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfevMoHIBgbP"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"ChilleD/StrategyQA\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the training split\n",
        "train_ds = ds[\"train\"]"
      ],
      "metadata": {
        "id": "3nXOp5QCCMxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0]"
      ],
      "metadata": {
        "id": "6kPIwj4Ew-qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM"
      ],
      "metadata": {
        "id": "2ceIKSwrsu2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ],
      "metadata": {
        "id": "dpldCVg60L9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google import genai\n",
        "from google.genai.types import GenerateContentConfig"
      ],
      "metadata": {
        "id": "Cf8Xn9ia0Lf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "daCJHjFf5rAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, model=MODEL, config=GENERATION_CONFIG):\n",
        "\n",
        "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "    config[\"system_instruction\"] = \"Once you are done finding the answer, only return Yes or No\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=prompt,\n",
        "        config=GenerateContentConfig(**config)\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    wall_time = end_time - start_time\n",
        "\n",
        "    new_response = {\n",
        "        'gemini_response': response,\n",
        "        'wall_time': wall_time\n",
        "    }\n",
        "\n",
        "    return new_response"
      ],
      "metadata": {
        "id": "aJKvz8tz0tql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "output_filename = \"./gemini_responses.jsonl\"\n",
        "all_responses = []\n",
        "\n",
        "for i in range(3):\n",
        "    response = generate(prompt=train_ds[i]['question'])\n",
        "\n",
        "    serializable_response = {}\n",
        "    try:\n",
        "        serializable_response = {\n",
        "            \"text\": response['gemini_response'].text.strip(),\n",
        "            \"total_token\": response['gemini_response'].usage_metadata.total_token_count,\n",
        "            \"wall_time\": response['wall_time']\n",
        "        }\n",
        "    except (AttributeError, KeyError, TypeError):\n",
        "        try:\n",
        "            serializable_response = response.to_dict()\n",
        "        except AttributeError:\n",
        "            serializable_response = str(response)\n",
        "\n",
        "    with open(output_filename, \"a\", encoding=\"utf-8\") as f:\n",
        "        json.dump(serializable_response, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    time.sleep(1)\n",
        "    print(f\"(Iteration {i+1} of {train_ds.shape[0]})\")\n",
        "\n",
        "print(\"Loop finished.\")"
      ],
      "metadata": {
        "id": "w1WkVXyVNaUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Native tool call"
      ],
      "metadata": {
        "id": "bR8zNLEA0g5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReAct Agent"
      ],
      "metadata": {
        "id": "JkMOsPAp0KMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List, Dict, Union, Callable\n",
        "\n",
        "import tiktoken\n",
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper, WikipediaAPIWrapper\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.schema import AgentAction, AgentFinish, LLMResult, BaseMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory\n",
        "from langchain.tools import Tool"
      ],
      "metadata": {
        "id": "NeBirELxHeM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReActAgentExecutor:\n",
        "    \"\"\"\n",
        "    A class to run the ReAct agent with specified configurations and tools.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = MODEL,\n",
        "        generation_config: Dict = GENERATION_CONFIG,\n",
        "        # safety_settings: Dict = SAFETY_SETTINGS,\n",
        "        max_iterations: int = MAX_ITERATIONS,\n",
        "        max_execution_time: int = MAX_EXECUTION_TIME,\n",
        "        google_api_key: str = GOOGLE_API_KEY,\n",
        "        cse_id: str = CSE_ID,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.generation_config = generation_config\n",
        "        # self.safety_settings = safety_settings\n",
        "        self.max_iterations = max_iterations\n",
        "        self.max_execution_time = max_execution_time\n",
        "        self.google_api_key = google_api_key\n",
        "        self.cse_id = cse_id\n",
        "        self.llm = None\n",
        "        self.tools = None\n",
        "        self.agent = None\n",
        "        self.agent_executor = None\n",
        "        self.token_callback = None\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_tools()\n",
        "        self._setup_agent()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "      \"\"\"Initializes the language model.\"\"\"\n",
        "      if not GEMINI_API_KEY or GEMINI_API_KEY == \"your_gemini_api_key\":\n",
        "          raise ValueError(\"GEMINI_API_KEY must be set to a valid API key.\")\n",
        "      self.llm = ChatGoogleGenerativeAI(\n",
        "          model=self.model,\n",
        "          google_api_key=GEMINI_API_KEY,\n",
        "          generation_config=self.generation_config,\n",
        "          # safety_settings=self.safety_settings,\n",
        "      )\n",
        "\n",
        "    def _setup_tools(self):\n",
        "        \"\"\"Sets up the tools for the agent.\"\"\"\n",
        "        search = GoogleSearchAPIWrapper(\n",
        "            google_api_key=self.google_api_key, google_cse_id=self.cse_id\n",
        "        )\n",
        "        # wikipedia = WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=1000)\n",
        "\n",
        "        self.tools = [\n",
        "            Tool(\n",
        "                name=\"Google Search\",\n",
        "                func=search.run,\n",
        "                description=\"Useful for finding information on current events, comparisons, or diverse perspectives.\",\n",
        "            ),\n",
        "            # Tool(\n",
        "            #     name=\"Wikipedia\",\n",
        "            #     func=wikipedia.run,\n",
        "            #     description=\"Useful for getting definitions and summaries of topics from Wikipedia.\",\n",
        "            # ),\n",
        "        ]\n",
        "\n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Sets up the ReAct agent and executor.\"\"\"\n",
        "        prompt = hub.pull(\"hwchase17/react\")\n",
        "        self.agent = create_react_agent(self.llm, self.tools, prompt)\n",
        "\n",
        "        self.token_callback = TokenCountingCallbackHandler(self.model)\n",
        "        self.agent_executor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=self.tools,\n",
        "            verbose=False,\n",
        "            handle_parsing_errors=True,\n",
        "            max_iterations=self.max_iterations,\n",
        "            max_execution_time=self.max_execution_time,\n",
        "            callbacks=[self.token_callback],\n",
        "        )\n",
        "\n",
        "    def run(self, input_data: Union[Dict, str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Runs the agent with the given input data.\n",
        "\n",
        "        Args:\n",
        "            input_data: Either a dictionary or a string representing the input for the agent.\n",
        "\n",
        "        Returns:\n",
        "            The output from the agent.\n",
        "        \"\"\"\n",
        "        if isinstance(input_data, str):\n",
        "            input_data = {\"input\": input_data}\n",
        "\n",
        "        try:\n",
        "            result = self.agent_executor.invoke(input_data)\n",
        "            # Include token usage information in the result\n",
        "            result[\"token_usage\"] = {\n",
        "                \"total_tokens\": self.token_callback.total_tokens,\n",
        "                \"prompt_tokens\": self.token_callback.prompt_tokens,\n",
        "                \"completion_tokens\": self.token_callback.completion_tokens,\n",
        "            }\n",
        "            self.token_callback.reset()  # Reset after each run\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "\n",
        "class TokenCountingCallbackHandler(BaseCallbackHandler):\n",
        "    \"\"\"Callback handler for counting tokens used by the language model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        self.total_tokens = 0\n",
        "        self.prompt_tokens = 0\n",
        "        self.completion_tokens = 0\n",
        "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    def on_llm_start(\n",
        "        self, serialized: Dict[str, any], prompts: List[str], **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"Collect prompt tokens when LLM starts.\"\"\"\n",
        "        for prompt in prompts:\n",
        "            self.prompt_tokens += len(self.encoding.encode(prompt))\n",
        "\n",
        "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
        "        \"\"\"Collect completion tokens when LLM finishes generating.\"\"\"\n",
        "        if response.generations:\n",
        "            for generation_list in response.generations:\n",
        "                for generation in generation_list:\n",
        "                    if generation.text:\n",
        "                        self.completion_tokens += len(\n",
        "                            self.encoding.encode(generation.text)\n",
        "                        )\n",
        "\n",
        "    def on_agent_action(self, action: AgentAction, **kwargs) -> None:\n",
        "        \"\"\"Increment token count on agent action.\"\"\"\n",
        "        if action.log:\n",
        "            self.total_tokens += len(self.encoding.encode(action.log))\n",
        "\n",
        "    def on_agent_finish(self, finish: AgentFinish, **kwargs) -> None:\n",
        "        \"\"\"Increment token count on agent finish.\"\"\"\n",
        "        if finish.log:\n",
        "            self.total_tokens += len(self.encoding.encode(finish.log))\n",
        "\n",
        "    def on_chain_end(self, outputs, **kwargs) -> None:\n",
        "        \"\"\"Print the total tokens used when the chain finishes.\"\"\"\n",
        "        self.total_tokens += self.completion_tokens + self.prompt_tokens\n",
        "        print(f\"Prompt tokens: {self.prompt_tokens}\")\n",
        "        print(f\"Completion tokens: {self.completion_tokens}\")\n",
        "        print(f\"Total tokens used in this chain: {self.total_tokens}\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the counters for the next chain run.\"\"\"\n",
        "        self.total_tokens = 0\n",
        "        self.prompt_tokens = 0\n",
        "        self.completion_tokens = 0"
      ],
      "metadata": {
        "id": "XhDsoEW8Hfxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agent_executor = ReActAgentExecutor()\n",
        "# result = agent_executor.run(train_ds[0][\"question\"])\n"
      ],
      "metadata": {
        "id": "dNmyUg2nH8nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "output_filename=\"agent_results.jsonl\"\n",
        "agent_executor = ReActAgentExecutor()\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for i, question in enumerate(train_ds[\"question\"]):\n",
        "    print(f\"Running agent for question {i+1}: {question}\")\n",
        "    result = agent_executor.run(question)\n",
        "    print(f\"Result for question {i+1}: {result}\")\n",
        "\n",
        "    all_results.append(\n",
        "        {\"question\": question, \"result\": result}\n",
        "    )\n",
        "\n",
        "    # Save the updated list to the file after each iteration\n",
        "    with open(output_filename, \"w\") as f:\n",
        "        for item in all_results:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "    print(f\"Results for question {i+1} saved to {output_filename}\")"
      ],
      "metadata": {
        "id": "KIqiODo1ImD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReWOO"
      ],
      "metadata": {
        "id": "aM_tfetUo5tr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReWOO: Decoupling Reasoning from Observations\n",
        "for Efficient Augmented Language Models [paper](https://arxiv.org/pdf/2305.18323)\n",
        "\n",
        "based on the implementation [here](https://github.com/billxbf/ReWOO/tree/main)"
      ],
      "metadata": {
        "id": "MwhdGb0u0e4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import re\n",
        "import tiktoken\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper"
      ],
      "metadata": {
        "id": "98wzequro7VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Simplified Worker ---\n",
        "class GoogleSearchWorker:\n",
        "    def __init__(self, name=\"Google\"):\n",
        "        self.name = name\n",
        "        self.google_api_key = GOOGLE_API_KEY\n",
        "        self.cse_id = CSE_ID\n",
        "        self.description = \"Worker that searches results from Google. Useful when you need to find short \" \\\n",
        "                           \"and succinct answers about a specific topic. Input should be a search query.\"\n",
        "\n",
        "    def run(self, input):\n",
        "        search = GoogleSearchAPIWrapper(\n",
        "            google_api_key=self.google_api_key, google_cse_id=self.cse_id\n",
        "        )\n",
        "        # Get the results from the API\n",
        "        results = search.results(input, 1)\n",
        "\n",
        "        # Print the structure of the results for debugging\n",
        "        print(\"Results Structure:\", results)\n",
        "\n",
        "        evidence = \"\"\n",
        "        for result in results:\n",
        "            # Check if 'snippet' exists, otherwise use 'title' or 'body'\n",
        "            if \"snippet\" in result:\n",
        "                evidence += result[\"snippet\"]\n",
        "            elif \"title\" in result:\n",
        "                evidence += result[\"title\"]\n",
        "            elif \"body\" in result:  # Use \"body\" as a fallback\n",
        "                evidence += result[\"body\"]\n",
        "            else:\n",
        "                print(\"Warning: No relevant information found in result:\", result)\n",
        "\n",
        "        return evidence\n",
        "\n",
        "# --- LLM Node (Simplified) ---\n",
        "class LLMNode:\n",
        "    def __init__(self, name, model_name, stop=None, input_type=str, output_type=str):\n",
        "        self.name = name\n",
        "        self.model_name = model_name\n",
        "        self.model = model_name\n",
        "        self.stop = stop\n",
        "        self.input_type = input_type\n",
        "        self.output_type = output_type\n",
        "        self.generation_config = {\n",
        "            \"temperature\": 0,\n",
        "         }\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=self.model,\n",
        "            google_api_key=GEMINI_API_KEY,\n",
        "            generation_config=self.generation_config,\n",
        "            # safety_settings=self.safety_settings,\n",
        "        )\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    def call_llm(self, prompt, stop):\n",
        "        if isinstance(prompt, list):\n",
        "            prompt_template = PromptTemplate(template=prompt[0], input_variables=[\"question\"])\n",
        "            prompt_text = prompt[0]\n",
        "            llm_chain = LLMChain(prompt=prompt_template, llm=self.llm, verbose=False)\n",
        "            response = llm_chain(prompt[1])\n",
        "            output = response[\"text\"].strip()\n",
        "        else:\n",
        "            prompt_template = PromptTemplate(template=prompt, input_variables=[])\n",
        "            prompt_text = prompt\n",
        "            llm_chain = LLMChain(prompt=prompt_template, llm=self.llm, verbose=False)\n",
        "            response = llm_chain({})\n",
        "            output = response[\"text\"].strip()\n",
        "\n",
        "        prompt_tokens = len(self.tokenizer.encode(prompt_text))\n",
        "        completion_tokens = len(self.tokenizer.encode(output))\n",
        "        return {\n",
        "            \"output\": output,\n",
        "            \"prompt_tokens\": prompt_tokens,\n",
        "            \"completion_tokens\": completion_tokens\n",
        "        }\n",
        "\n",
        "# --- Planner ---\n",
        "class Planner(LLMNode):\n",
        "    def __init__(self, model_name=\"gemini-pro\", fewshot=\"\"):\n",
        "        super().__init__(\"Planner\", model_name, stop=None, input_type=str, output_type=str)\n",
        "        self.worker_prompt = \"Tools can be one of the following:\\nGoogle[input]: Worker that searches results from Google. Useful when you need to find short and succinct answers about a specific topic. Input should be a search query.\\n\\n\"\n",
        "        self.prefix = \"For the following tasks, make plans that can solve the problem step-by-step. For each plan, \" \\\n",
        "                     \"indicate which external tool together with tool input to retrieve evidence. You can store the \" \\\n",
        "                     \"evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...) \\n\\n\"\n",
        "        self.suffix = \"Begin! Describe your plans with rich details. Each Plan should be followed by only one #E.\\n\\n\"\n",
        "        self.fewshot = fewshot\n",
        "\n",
        "    def run(self, input, log=False):\n",
        "        prompt = self.prefix + self.worker_prompt + self.fewshot + self.suffix + input + '\\n'\n",
        "        response = self.call_llm(prompt, self.stop)\n",
        "        if log:\n",
        "            return response\n",
        "        return response[\"output\"]\n",
        "\n",
        "# --- Solver ---\n",
        "class Solver(LLMNode):\n",
        "    def __init__(self, model_name=\"gemini-pro\"):\n",
        "        super().__init__(\"Solver\", model_name, stop=None, input_type=str, output_type=str)\n",
        "        self.prefix = \"Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\\n\\n\"\n",
        "        self.suffix = \"\\nNow begin to solve the task or problem. Respond with the answer directly with no extra words.\\n\\n\"\n",
        "\n",
        "    def run(self, input, worker_log, log=False):\n",
        "        prompt = self.prefix + input + \"\\n\" + worker_log + self.suffix + input + '\\n'\n",
        "        response = self.call_llm(prompt, self.stop)\n",
        "        if log:\n",
        "            return response\n",
        "        return response[\"output\"]\n",
        "\n",
        "# --- Main PWS Class ---\n",
        "class PWS:\n",
        "    def __init__(self, planner_model=\"gemini-pro\", solver_model=\"gemini-pro\", fewshot=\"\"):\n",
        "        self.worker = GoogleSearchWorker()\n",
        "        self.planner = Planner(model_name=planner_model, fewshot=fewshot)\n",
        "        self.solver = Solver(model_name=solver_model)\n",
        "        self.plans = []\n",
        "        self.planner_evidences = {}\n",
        "        self.worker_evidences = {}\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    def run(self, input):\n",
        "        self._reinitialize()\n",
        "        result = {}\n",
        "        st = time.time()\n",
        "\n",
        "        # Plan\n",
        "        planner_response = self.planner.run(input, log=True)\n",
        "        plan = planner_response[\"output\"]\n",
        "\n",
        "        # Store the planner input before calling call_llm\n",
        "        planner_input = self.planner.prefix + self.planner.worker_prompt + self.planner.fewshot + self.planner.suffix + input + '\\n'\n",
        "\n",
        "        planner_log = planner_input + planner_response[\"output\"]\n",
        "\n",
        "        self.plans = self._parse_plans(plan)\n",
        "        self.planner_evidences = self._parse_planner_evidences(plan)\n",
        "\n",
        "        # --- Validation and Error Handling ---\n",
        "        valid_plan = self._validate_plan()\n",
        "        if not valid_plan:\n",
        "            print(\"Warning: Invalid plan generated. Skipping worker and passing the question to solver.\")\n",
        "            worker_log = \"\"\n",
        "            solver_input = self.solver.prefix + input + \"\\n\" + self.solver.suffix + input + '\\n'\n",
        "        else:\n",
        "            # Work\n",
        "            self._get_worker_evidences()\n",
        "            worker_log = \"\"\n",
        "            total_worker_tokens = 0\n",
        "            for i in range(len(self.plans)):\n",
        "                e = f\"#E{i + 1}\"\n",
        "                if e in self.worker_evidences:\n",
        "                    worker_log += f\"{self.plans[i]}\\nEvidence:\\n{self.worker_evidences[e]}\\n\"\n",
        "                    total_worker_tokens += self._count_tokens(self.worker_evidences[e])\n",
        "                else:\n",
        "                    worker_log += f\"{self.plans[i]}\\nEvidence:\\nNo evidence found for {e}\\n\"\n",
        "                    print(f\"Warning: No evidence found for {e} in self.worker_evidences\")\n",
        "\n",
        "        # Solve\n",
        "        solver_response = self.solver.run(input, worker_log, log=True)\n",
        "\n",
        "        # Similar fix for solver_log\n",
        "        if valid_plan:\n",
        "            solver_input = self.solver.prefix + input + \"\\n\" + worker_log + self.solver.suffix + input + '\\n'\n",
        "        else:\n",
        "            solver_input = self.solver.prefix + input + \"\\n\" + self.solver.suffix + input + '\\n'\n",
        "\n",
        "        output = solver_response[\"output\"]\n",
        "        solver_log = solver_input + solver_response[\"output\"]\n",
        "\n",
        "        result[\"wall_time\"] = time.time() - st\n",
        "        result[\"input\"] = input\n",
        "        result[\"output\"] = output\n",
        "        result[\"planner_log\"] = planner_log\n",
        "        result[\"worker_log\"] = worker_log\n",
        "        result[\"solver_log\"] = solver_log\n",
        "        result[\"steps\"] = len(self.plans) + 1\n",
        "        result[\"prompt_tokens\"] = planner_response[\"prompt_tokens\"] + solver_response[\"prompt_tokens\"]\n",
        "        result[\"completion_tokens\"] = planner_response[\"completion_tokens\"] + solver_response[\"completion_tokens\"]\n",
        "\n",
        "        if valid_plan:\n",
        "            result[\"total_tokens\"] = result[\"prompt_tokens\"] + result[\"completion_tokens\"] + total_worker_tokens\n",
        "        else:\n",
        "            result[\"total_tokens\"] = result[\"prompt_tokens\"] + result[\"completion_tokens\"]\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _validate_plan(self):\n",
        "        \"\"\"\n",
        "        Validates if the generated plan has the correct #E notation in sequence.\n",
        "        \"\"\"\n",
        "        for i in range(len(self.plans)):\n",
        "            expected_evidence_key = f\"#E{i + 1}\"\n",
        "            if expected_evidence_key not in self.planner_evidences:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def _parse_plans(self, response):\n",
        "        plans = []\n",
        "        for line in response.splitlines():\n",
        "            if line.startswith(\"Plan:\"):\n",
        "                plans.append(line)\n",
        "        return plans\n",
        "\n",
        "    def _parse_planner_evidences(self, response):\n",
        "        evidences = {}\n",
        "        for line in response.splitlines():\n",
        "            if line.startswith(\"#\") and line[1] == \"E\" and line[2].isdigit():\n",
        "                parts = line.split(\"=\", 1)  # Split into at most 2 parts\n",
        "                if len(parts) == 2:\n",
        "                    e, tool_call = parts\n",
        "                    e, tool_call = e.strip(), tool_call.strip()\n",
        "                    evidences[e] = tool_call\n",
        "                else:\n",
        "                    # Handle cases where there's no '=' after #E\n",
        "                    e = parts[0].strip()\n",
        "                    evidences[e] = \"No evidence found\"  # Or some other default value\n",
        "                    print(f\"Warning: Invalid planner evidence format: {line}\")\n",
        "        return evidences\n",
        "\n",
        "    def _get_worker_evidences(self):\n",
        "        for e, tool_call in self.planner_evidences.items():\n",
        "            if not tool_call.startswith(\"Google[\"):\n",
        "                self.worker_evidences[e] = \"No evidence found\"\n",
        "                continue\n",
        "            tool_input = tool_call[7:-1]\n",
        "            for var in re.findall(r\"#E\\d+\", tool_input):\n",
        "                if var in self.worker_evidences:\n",
        "                    tool_input = tool_input.replace(var, \"[\" + self.worker_evidences[var] + \"]\")\n",
        "\n",
        "            self.worker_evidences[e] = self.worker.run(tool_input)\n",
        "\n",
        "    def _reinitialize(self):\n",
        "        self.plans = []\n",
        "        self.planner_evidences = {}\n",
        "        self.worker_evidences = {}\n",
        "\n",
        "    def _count_tokens(self, text):\n",
        "        return len(self.tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "MMDEkPJrulcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "pws = PWS()\n",
        "\n",
        "results_file = \"./rewoo_results.json\"\n",
        "with open(results_file, \"w\") as f:\n",
        "    json.dump([], f)\n",
        "\n",
        "for question in train_ds[\"question\"]:\n",
        "    result = pws.run(question)\n",
        "    print(result)\n",
        "\n",
        "    with open(results_file, \"r+\") as f:\n",
        "        data = json.load(f)\n",
        "        data.append(result)\n",
        "        f.seek(0)\n",
        "        json.dump(data, f, indent=4)"
      ],
      "metadata": {
        "id": "vAqF24N_vF9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "\n",
        "[x] add baseline\n",
        "\n",
        "[] add prompt, completion, total tokens\n",
        "\n",
        "[] native google search retrieval\n",
        "\n",
        "[x] callback/count tokens for react\n",
        "\n",
        "[] ReWOO\n",
        "\n",
        "[ ] add walltime to all other functions -> # token, time, accuracy\n",
        "\n",
        "[] add everything to a class, run queries at concurrently"
      ],
      "metadata": {
        "id": "6AJANtdgltx6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNR-ttnzyPeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}