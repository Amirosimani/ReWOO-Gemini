{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXOYrLYKqBlMZEZnHZR1og",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirosimani/ReWOO-Gemini/blob/main/ReWOO_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n"
      ],
      "metadata": {
        "id": "YJPLxIdz2WXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|||\n",
        "|----------|-------------|\n",
        "| Author(s)   | amirimani@ |\n",
        "| Last updated | 24/01/2025 |\n",
        "<br><br>\n"
      ],
      "metadata": {
        "id": "boZaMQp4J2kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet datasets\n",
        "!pip install --quiet langchain langchain_community\n",
        "!pip install --quiet langchain_google_genai langchain_google_community\n",
        "!pip install --quiet wikipedia\n",
        "!pip install --quiet tiktoken"
      ],
      "metadata": {
        "id": "_In1NIeyqmD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import time\n",
        "import tiktoken\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from typing import List, Dict, Union, Callable, Any\n",
        "\n",
        "from langchain import hub\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "from langchain.schema import AgentAction, AgentFinish, LLMResult, BaseMessage\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.tools import Tool\n",
        "\n",
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from google.genai.types import GenerateContentConfig"
      ],
      "metadata": {
        "id": "K9cZba0Jv34Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "MODEL = \"gemini-1.5-flash\"\n",
        "MAX_ITERATIONS = 5\n",
        "MAX_EXECUTION_TIME = 45\n",
        "\n",
        "GENERATION_CONFIG = {\n",
        "    \"temperature\": 0.8,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 20,\n",
        "    \"candidate_count\": 1,\n",
        "    \"max_output_tokens\": 8192,\n",
        "}\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI')\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE-API')\n",
        "CSE_ID = userdata.get(\"CSE-ID\")"
      ],
      "metadata": {
        "id": "MRqm03Sz5_fF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "[**StrategyQA**](https://paperswithcode.com/dataset/strategyqa) is a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. It includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies.\n",
        "\n"
      ],
      "metadata": {
        "id": "pQxf0kNGBtVN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pfevMoHIBgbP"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"ChilleD/StrategyQA\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the training split\n",
        "train_ds = ds[\"train\"]"
      ],
      "metadata": {
        "id": "3nXOp5QCCMxp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds[0]"
      ],
      "metadata": {
        "id": "6kPIwj4Ew-qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9d47d7-401e-483a-87dd-b402087040c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'qid': '4fd64bb6ce5b78ab20b6',\n",
              " 'term': 'Mixed martial arts',\n",
              " 'description': 'full contact combat sport',\n",
              " 'question': 'Is Mixed martial arts totally original from Roman Colosseum games?',\n",
              " 'answer': False,\n",
              " 'facts': 'Mixed Martial arts in the UFC takes place in an enclosed structure called The Octagon. The Roman Colosseum games were fought in enclosed arenas where combatants would fight until the last man was standing. Mixed martial arts contests are stopped when one of the combatants is incapacitated. The Roman Colosseum was performed in front of crowds that numbered in the tens of thousands. Over 56,000 people attended UFC 193.'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM"
      ],
      "metadata": {
        "id": "2ceIKSwrsu2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "\n",
        "Use Gemnini with no tools and not fancy prompts!"
      ],
      "metadata": {
        "id": "dpldCVg60L9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "daCJHjFf5rAT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, model=MODEL, config=GENERATION_CONFIG):\n",
        "\n",
        "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "    config[\"system_instruction\"] = \"Once you are done finding the answer, only return Yes or No\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=prompt,\n",
        "        config=GenerateContentConfig(**config)\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    wall_time = end_time - start_time\n",
        "\n",
        "    result = {\n",
        "        'input': prompt,\n",
        "        'output': response.text.strip(),\n",
        "        'total_token': response.usage_metadata.total_token_count,\n",
        "        'wall_time': wall_time\n",
        "    }\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "aJKvz8tz0tql"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate(train_ds[1]['question'])"
      ],
      "metadata": {
        "id": "iwrw4FmJnH4J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReAct Agent"
      ],
      "metadata": {
        "id": "JkMOsPAp0KMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReActAgentExecutor:\n",
        "    \"\"\"\n",
        "    A class to run the ReAct agent with specified configurations and tools.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: str = MODEL,\n",
        "        generation_config: Dict = GENERATION_CONFIG,\n",
        "        max_iterations: int = MAX_ITERATIONS,\n",
        "        max_execution_time: int = MAX_EXECUTION_TIME,\n",
        "        google_api_key: str = GOOGLE_API_KEY,\n",
        "        cse_id: str = CSE_ID,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.generation_config = generation_config\n",
        "        # self.safety_settings = safety_settings\n",
        "        self.max_iterations = max_iterations\n",
        "        self.max_execution_time = max_execution_time\n",
        "        self.google_api_key = google_api_key\n",
        "        self.cse_id = cse_id\n",
        "        self.llm = None\n",
        "        self.tools = None\n",
        "        self.agent = None\n",
        "        self.agent_executor = None\n",
        "        self.token_callback = None\n",
        "\n",
        "        self._setup_llm()\n",
        "        self._setup_tools()\n",
        "        self._setup_agent()\n",
        "\n",
        "    def _setup_llm(self):\n",
        "        \"\"\"Initializes the language model.\"\"\"\n",
        "        if not GEMINI_API_KEY or GEMINI_API_KEY == \"your_gemini_api_key\":\n",
        "            raise ValueError(\"GEMINI_API_KEY must be set to a valid API key.\")\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=self.model,\n",
        "            google_api_key=GEMINI_API_KEY,\n",
        "            generation_config=self.generation_config,\n",
        "        )\n",
        "\n",
        "    def _setup_tools(self):\n",
        "        \"\"\"Sets up the tools for the agent.\"\"\"\n",
        "        search = GoogleSearchAPIWrapper(\n",
        "            google_api_key=self.google_api_key, google_cse_id=self.cse_id\n",
        "        )\n",
        "\n",
        "        self.tools = [\n",
        "            Tool(\n",
        "                name=\"Google Search\",\n",
        "                func=search.run,\n",
        "                description=\"Useful for finding information on current events, comparisons, or diverse perspectives.\",\n",
        "            ),\n",
        "        ]\n",
        "\n",
        "    def _setup_agent(self):\n",
        "        \"\"\"Sets up the ReAct agent and executor.\"\"\"\n",
        "        prompt = hub.pull(\"hwchase17/react\")\n",
        "        system_instruction = \"Once you are done finding the answer, only return Yes or No\"\n",
        "        prompt.template = system_instruction + \"\\n\" + prompt.template\n",
        "\n",
        "        self.agent = create_react_agent(self.llm, self.tools, prompt)\n",
        "\n",
        "        self.token_callback = TokenCountingCallbackHandler(self.model)\n",
        "        self.agent_executor = AgentExecutor(\n",
        "            agent=self.agent,\n",
        "            tools=self.tools,\n",
        "            verbose=False,\n",
        "            handle_parsing_errors=True,\n",
        "            max_iterations=self.max_iterations,\n",
        "            max_execution_time=self.max_execution_time,\n",
        "            callbacks=[self.token_callback],\n",
        "        )\n",
        "\n",
        "    def run(self, input_data: Union[Dict, str]) -> Dict:\n",
        "        \"\"\"\n",
        "        Runs the agent with the given input data.\n",
        "\n",
        "        Args:\n",
        "            input_data: Either a dictionary or a string representing the input for the agent.\n",
        "\n",
        "        Returns:\n",
        "            The output from the agent.\n",
        "        \"\"\"\n",
        "        if isinstance(input_data, str):\n",
        "            input_data = {\"input\": input_data}\n",
        "\n",
        "        start_time = time.time()  # Start timing\n",
        "        try:\n",
        "            result = self.agent_executor.invoke(input_data)\n",
        "            # Include token usage information in the result\n",
        "            result[\"total_token\"] = self.token_callback.total_token\n",
        "\n",
        "            self.token_callback.reset()  # Reset after each run\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            result = {\"error\": str(e)}\n",
        "        end_time = time.time()  # End timing\n",
        "\n",
        "        # Log wall time\n",
        "        wall_time = end_time - start_time\n",
        "        print(f\"Wall time for execution: {wall_time:.2f} seconds\")\n",
        "        result[\"wall_time\"] = wall_time\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "class TokenCountingCallbackHandler(BaseCallbackHandler):\n",
        "    \"\"\"Callback handler for counting tokens used by the language model.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "        self.total_token = 0\n",
        "        self.prompt_tokens = 0\n",
        "        self.completion_tokens = 0\n",
        "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    def on_llm_start(\n",
        "        self, serialized: Dict[str, any], prompts: List[str], **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"Collect prompt tokens when LLM starts.\"\"\"\n",
        "        for prompt in prompts:\n",
        "            self.prompt_tokens += len(self.encoding.encode(prompt))\n",
        "\n",
        "    def on_llm_end(self, response: LLMResult, **kwargs) -> None:\n",
        "        \"\"\"Collect completion tokens when LLM finishes generating.\"\"\"\n",
        "        if response.generations:\n",
        "            for generation_list in response.generations:\n",
        "                for generation in generation_list:\n",
        "                    if generation.text:\n",
        "                        self.completion_tokens += len(\n",
        "                            self.encoding.encode(generation.text)\n",
        "                        )\n",
        "\n",
        "    def on_agent_action(self, action: AgentAction, **kwargs) -> None:\n",
        "        \"\"\"Increment token count on agent action.\"\"\"\n",
        "        if action.log:\n",
        "            self.total_token += len(self.encoding.encode(action.log))\n",
        "\n",
        "    def on_agent_finish(self, finish: AgentFinish, **kwargs) -> None:\n",
        "        \"\"\"Increment token count on agent finish.\"\"\"\n",
        "        if finish.log:\n",
        "            self.total_token += len(self.encoding.encode(finish.log))\n",
        "\n",
        "    def on_chain_end(self, outputs, **kwargs) -> None:\n",
        "        \"\"\"Print the total tokens used when the chain finishes.\"\"\"\n",
        "        self.total_token += self.completion_tokens + self.prompt_tokens\n",
        "        print(f\"Prompt tokens: {self.prompt_tokens}\")\n",
        "        print(f\"Completion tokens: {self.completion_tokens}\")\n",
        "        print(f\"Total tokens used in this chain: {self.total_token}\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the counters for the next chain run.\"\"\"\n",
        "        self.total_token = 0\n",
        "        self.prompt_tokens = 0\n",
        "        self.completion_tokens = 0\n"
      ],
      "metadata": {
        "id": "XhDsoEW8Hfxl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agent_executor = ReActAgentExecutor()\n",
        "# result = agent_executor.run(train_ds[1][\"question\"])"
      ],
      "metadata": {
        "id": "dNmyUg2nH8nM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReWOO"
      ],
      "metadata": {
        "id": "aM_tfetUo5tr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReWOO: Decoupling Reasoning from Observations\n",
        "for Efficient Augmented Language Models [paper](https://arxiv.org/pdf/2305.18323)\n",
        "\n",
        "based on the implementation [here](https://github.com/billxbf/ReWOO/tree/main)"
      ],
      "metadata": {
        "id": "MwhdGb0u0e4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Simplified Worker ---\n",
        "class GoogleSearchWorker:\n",
        "    def __init__(self, name=\"Google\"):\n",
        "        self.name = name\n",
        "        self.google_api_key = GOOGLE_API_KEY\n",
        "        self.cse_id = CSE_ID\n",
        "        self.description = \"Worker that searches results from Google. Useful when you need to find short \" \\\n",
        "                           \"and succinct answers about a specific topic. Input should be a search query.\"\n",
        "\n",
        "    def run(self, input):\n",
        "        search = GoogleSearchAPIWrapper(\n",
        "            google_api_key=self.google_api_key, google_cse_id=self.cse_id\n",
        "        )\n",
        "        # Get the results from the API\n",
        "        results = search.results(input, 1)\n",
        "\n",
        "        # Print the structure of the results for debugging\n",
        "        print(\"Results Structure:\", results)\n",
        "\n",
        "        evidence = \"\"\n",
        "        for result in results:\n",
        "            # Check if 'snippet' exists, otherwise use 'title' or 'body'\n",
        "            if \"snippet\" in result:\n",
        "                evidence += result[\"snippet\"]\n",
        "            elif \"title\" in result:\n",
        "                evidence += result[\"title\"]\n",
        "            elif \"body\" in result:  # Use \"body\" as a fallback\n",
        "                evidence += result[\"body\"]\n",
        "            else:\n",
        "                print(\"Warning: No relevant information found in result:\", result)\n",
        "\n",
        "        return evidence\n",
        "\n",
        "# --- LLM Node (Simplified) ---\n",
        "class LLMNode:\n",
        "    def __init__(self, name, model_name, stop=None, input_type=str, output_type=str):\n",
        "        self.name = name\n",
        "        self.model_name = model_name\n",
        "        self.model = model_name\n",
        "        self.stop = stop\n",
        "        self.input_type = input_type\n",
        "        self.output_type = output_type\n",
        "        self.generation_config = GENERATION_CONFIG\n",
        "        self.llm = ChatGoogleGenerativeAI(\n",
        "            model=self.model,\n",
        "            google_api_key=GEMINI_API_KEY,\n",
        "            generation_config=self.generation_config,\n",
        "            # safety_settings=self.safety_settings,\n",
        "        )\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "    def call_llm(self, prompt, stop):\n",
        "        if isinstance(prompt, list):\n",
        "            prompt_template = PromptTemplate(template=prompt[0], input_variables=[\"question\"])\n",
        "            prompt_text = prompt[0]\n",
        "            llm_chain = LLMChain(prompt=prompt_template, llm=self.llm, verbose=False)\n",
        "            response = llm_chain(prompt[1])\n",
        "            output = response[\"text\"].strip()\n",
        "        else:\n",
        "            prompt_template = PromptTemplate(template=prompt, input_variables=[])\n",
        "            prompt_text = prompt\n",
        "            llm_chain = LLMChain(prompt=prompt_template, llm=self.llm, verbose=False)\n",
        "            response = llm_chain({})\n",
        "            output = response[\"text\"].strip()\n",
        "\n",
        "        prompt_tokens = len(self.tokenizer.encode(prompt_text))\n",
        "        completion_tokens = len(self.tokenizer.encode(output))\n",
        "        return {\n",
        "            \"output\": output,\n",
        "            \"prompt_tokens\": prompt_tokens,\n",
        "            \"completion_tokens\": completion_tokens\n",
        "        }\n",
        "\n",
        "# --- Planner ---\n",
        "class Planner(LLMNode):\n",
        "    def __init__(self, model_name=\"gemini-pro\", fewshot=\"\"):\n",
        "        super().__init__(\"Planner\", model_name, stop=None, input_type=str, output_type=str)\n",
        "        self.worker_prompt = \"Tools can be one of the following:\\nGoogle[input]: Worker that searches results from Google. Useful when you need to find short and succinct answers about a specific topic. Input should be a search query.\\n\\n\"\n",
        "        self.prefix = \"For the following tasks, make plans that can solve the problem step-by-step. For each plan, \" \\\n",
        "                     \"indicate which external tool together with tool input to retrieve evidence. You can store the \" \\\n",
        "                     \"evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...) \\n\\n\"\n",
        "        self.suffix = \"Begin! Describe your plans with rich details. Each Plan should be followed by only one #E.\\n\\n\"\n",
        "        self.fewshot = fewshot\n",
        "\n",
        "    def run(self, input, log=False):\n",
        "        prompt = self.prefix + self.worker_prompt + self.fewshot + self.suffix + input + '\\n'\n",
        "        response = self.call_llm(prompt, self.stop)\n",
        "        if log:\n",
        "            return response\n",
        "        return response[\"output\"]\n",
        "\n",
        "# --- Solver ---\n",
        "class Solver(LLMNode):\n",
        "    def __init__(self, model_name=\"gemini-pro\"):\n",
        "        super().__init__(\"Solver\", model_name, stop=None, input_type=str, output_type=str)\n",
        "        self.prefix = \"Solve the following task or problem. To assist you, we provide some plans and corresponding evidences that might be helpful. Notice that some of these information contain noise so you should trust them with caution.\\n\\n\"\n",
        "        self.suffix = \"\\nNow begin to solve the task or problem. Respond with the answer directly with no extra words.\\n\\n\"\n",
        "\n",
        "    def run(self, input, worker_log, log=False):\n",
        "        prompt = self.prefix + input + \"\\n\" + worker_log + self.suffix + input + '\\n'\n",
        "        response = self.call_llm(prompt, self.stop)\n",
        "        if log:\n",
        "            return response\n",
        "        return response[\"output\"]\n",
        "\n",
        "# --- Main PWS Class ---\n",
        "class PWS:\n",
        "    def __init__(self, planner_model=\"gemini-pro\", solver_model=\"gemini-pro\", fewshot=\"\"):\n",
        "        self.worker = GoogleSearchWorker()\n",
        "        self.planner = Planner(model_name=planner_model, fewshot=fewshot)\n",
        "        self.solver = Solver(model_name=solver_model)\n",
        "        self.plans = []\n",
        "        self.planner_evidences = {}\n",
        "        self.worker_evidences = {}\n",
        "        self.tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    def run(self, input):\n",
        "        self._reinitialize()\n",
        "        result = {}\n",
        "        st = time.time()\n",
        "\n",
        "        # Plan\n",
        "        planner_response = self.planner.run(input, log=True)\n",
        "        plan = planner_response[\"output\"]\n",
        "\n",
        "        # Store the planner input before calling call_llm\n",
        "        planner_input = self.planner.prefix + self.planner.worker_prompt + self.planner.fewshot + self.planner.suffix + input + '\\n'\n",
        "\n",
        "        planner_log = planner_input + planner_response[\"output\"]\n",
        "\n",
        "        self.plans = self._parse_plans(plan)\n",
        "        self.planner_evidences = self._parse_planner_evidences(plan)\n",
        "\n",
        "        # --- Validation and Error Handling ---\n",
        "        valid_plan = self._validate_plan()\n",
        "        if not valid_plan:\n",
        "            print(\"Warning: Invalid plan generated. Skipping worker and passing the question to solver.\")\n",
        "            worker_log = \"\"\n",
        "            solver_input = self.solver.prefix + input + \"\\n\" + self.solver.suffix + input + '\\n'\n",
        "        else:\n",
        "            # Work\n",
        "            self._get_worker_evidences()\n",
        "            worker_log = \"\"\n",
        "            total_worker_tokens = 0\n",
        "            for i in range(len(self.plans)):\n",
        "                e = f\"#E{i + 1}\"\n",
        "                if e in self.worker_evidences:\n",
        "                    worker_log += f\"{self.plans[i]}\\nEvidence:\\n{self.worker_evidences[e]}\\n\"\n",
        "                    total_worker_tokens += self._count_tokens(self.worker_evidences[e])\n",
        "                else:\n",
        "                    worker_log += f\"{self.plans[i]}\\nEvidence:\\nNo evidence found for {e}\\n\"\n",
        "                    print(f\"Warning: No evidence found for {e} in self.worker_evidences\")\n",
        "\n",
        "        # Solve\n",
        "        solver_response = self.solver.run(input, worker_log, log=True)\n",
        "\n",
        "        # Similar fix for solver_log\n",
        "        if valid_plan:\n",
        "            solver_input = self.solver.prefix + input + \"\\n\" + worker_log + self.solver.suffix + input + '\\n'\n",
        "        else:\n",
        "            solver_input = self.solver.prefix + input + \"\\n\" + self.solver.suffix + input + '\\n'\n",
        "\n",
        "        output = solver_response[\"output\"]\n",
        "        solver_log = solver_input + solver_response[\"output\"]\n",
        "\n",
        "        result[\"wall_time\"] = time.time() - st\n",
        "        result[\"input\"] = input\n",
        "        result[\"output\"] = output\n",
        "        result[\"planner_log\"] = planner_log\n",
        "        result[\"worker_log\"] = worker_log\n",
        "        result[\"solver_log\"] = solver_log\n",
        "        result[\"steps\"] = len(self.plans) + 1\n",
        "        result[\"prompt_tokens\"] = planner_response[\"prompt_tokens\"] + solver_response[\"prompt_tokens\"]\n",
        "        result[\"completion_tokens\"] = planner_response[\"completion_tokens\"] + solver_response[\"completion_tokens\"]\n",
        "\n",
        "        if valid_plan:\n",
        "            result[\"total_token\"] = result[\"completion_tokens\"] + total_worker_tokens\n",
        "        else:\n",
        "            result[\"total_token\"] = result[\"completion_tokens\"]\n",
        "\n",
        "\n",
        "        result = {k: result[k] for k in [\"input\", \"output\", \"wall_time\", \"total_token\"]}\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _validate_plan(self):\n",
        "        \"\"\"\n",
        "        Validates if the generated plan has the correct #E notation in sequence.\n",
        "        \"\"\"\n",
        "        for i in range(len(self.plans)):\n",
        "            expected_evidence_key = f\"#E{i + 1}\"\n",
        "            if expected_evidence_key not in self.planner_evidences:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def _parse_plans(self, response):\n",
        "        plans = []\n",
        "        for line in response.splitlines():\n",
        "            if line.startswith(\"Plan:\"):\n",
        "                plans.append(line)\n",
        "        return plans\n",
        "\n",
        "    def _parse_planner_evidences(self, response):\n",
        "        evidences = {}\n",
        "        for line in response.splitlines():\n",
        "            if line.startswith(\"#\") and line[1] == \"E\" and line[2].isdigit():\n",
        "                parts = line.split(\"=\", 1)  # Split into at most 2 parts\n",
        "                if len(parts) == 2:\n",
        "                    e, tool_call = parts\n",
        "                    e, tool_call = e.strip(), tool_call.strip()\n",
        "                    evidences[e] = tool_call\n",
        "                else:\n",
        "                    # Handle cases where there's no '=' after #E\n",
        "                    e = parts[0].strip()\n",
        "                    evidences[e] = \"No evidence found\"  # Or some other default value\n",
        "                    print(f\"Warning: Invalid planner evidence format: {line}\")\n",
        "        return evidences\n",
        "\n",
        "    def _get_worker_evidences(self):\n",
        "        for e, tool_call in self.planner_evidences.items():\n",
        "            if not tool_call.startswith(\"Google[\"):\n",
        "                self.worker_evidences[e] = \"No evidence found\"\n",
        "                continue\n",
        "            tool_input = tool_call[7:-1]\n",
        "            for var in re.findall(r\"#E\\d+\", tool_input):\n",
        "                if var in self.worker_evidences:\n",
        "                    tool_input = tool_input.replace(var, \"[\" + self.worker_evidences[var] + \"]\")\n",
        "\n",
        "            self.worker_evidences[e] = self.worker.run(tool_input)\n",
        "\n",
        "    def _reinitialize(self):\n",
        "        self.plans = []\n",
        "        self.planner_evidences = {}\n",
        "        self.worker_evidences = {}\n",
        "\n",
        "    def _count_tokens(self, text):\n",
        "        return len(self.tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "MMDEkPJrulcx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rewoo_executor = PWS()\n",
        "result = rewoo_executor.run(train_ds[1][\"question\"])\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfK93ZR2qSBx",
        "outputId": "c6a22d0e-8868-4f60-85e6-96499b87657d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-c01d2edc448a>:62: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(prompt=prompt_template, llm=self.llm, verbose=False)\n",
            "<ipython-input-12-c01d2edc448a>:63: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm_chain({})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Invalid plan generated. Skipping worker and passing the question to solver.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Is the cuisine of Hawaii suitable for a vegan?',\n",
              " 'output': 'Yes',\n",
              " 'wall_time': 2.539553165435791,\n",
              " 'total_token': 74}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run evalaution"
      ],
      "metadata": {
        "id": "PZjBg0aDvgrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(data: List[Dict[str, Any]]) -> Dict[int, Dict[str, Any]]:\n",
        "    \"\"\"Processes data and runs three functions, storing results in a dictionary.\"\"\"\n",
        "\n",
        "    results = {}\n",
        "    question = data[\"question\"]\n",
        "\n",
        "    generate_result = generate(question)\n",
        "    agent_result = ReActAgentExecutor().run(question)\n",
        "    rewoo_result = PWS().run(question)\n",
        "\n",
        "    return {\n",
        "        \"gemini\": generate_result,\n",
        "        \"agent_executor\": agent_result,\n",
        "        \"rewoo_executor\": rewoo_result,\n",
        "    }"
      ],
      "metadata": {
        "id": "6Incv-aTvfwW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('./results.jsonl', \"a\") as file:  # Open file in append mode\n",
        "    for data in train_ds:\n",
        "        result = run_experiment(data)\n",
        "        json.dump(result, file)\n",
        "        file.write(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6S_dje1xadW",
        "outputId": "b0cdb2db-9afd-4bde-cd21-7b29e4f51189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 56\n",
            "Wall time for execution: 2.00 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 51\n",
            "Wall time for execution: 1.98 seconds\n",
            "Warning: Invalid planner evidence format: #E1: Search results from Google for \"Vegan Hawaiian cuisine\"\n",
            "Warning: Invalid planner evidence format: #E2: List of Hawaiian restaurants with vegan menu options\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 51\n",
            "Wall time for execution: 2.10 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 69\n",
            "Wall time for execution: 2.37 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 58\n",
            "Wall time for execution: 2.00 seconds\n",
            "Warning: Invalid planner evidence format: #E1: Google[\"Common name of Solanum melongena in Mumbai\"]\n",
            "Warning: Invalid plan generated. Skipping worker and passing the question to solver.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 137\n",
            "Wall time for execution: 2.67 seconds\n",
            "Results Structure: [{'title': 'ChilleD/StrategyQA · Datasets at Hugging Face', 'link': 'https://huggingface.co/datasets/ChilleD/StrategyQA', 'snippet': 'Do flying fish have good eyesight? true. Flying fish are commonly found in the epipelagic zone, the top layer of the ocean to a depth of about 200 m (656 ft)\\xa0...'}]\n",
            "Results Structure: [{'Result': 'No good Google Search Result was found'}]\n",
            "Warning: No relevant information found in result: {'Result': 'No good Google Search Result was found'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 29\n",
            "Wall time for execution: 0.47 seconds\n",
            "Warning: Invalid plan generated. Skipping worker and passing the question to solver.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 54\n",
            "Wall time for execution: 0.71 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 52\n",
            "Wall time for execution: 2.19 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 93\n",
            "Wall time for execution: 1.69 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 43\n",
            "Wall time for execution: 1.88 seconds\n",
            "Warning: Invalid plan generated. Skipping worker and passing the question to solver.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 57\n",
            "Wall time for execution: 0.74 seconds\n",
            "Warning: Invalid plan generated. Skipping worker and passing the question to solver.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 67\n",
            "Wall time for execution: 2.25 seconds\n",
            "Results Structure: [{'title': \"On a 'Cresta' of a wave – down memory lane to the retro sweetshop ...\", 'link': 'https://nowrigglingoutofwriting.wordpress.com/2010/09/04/on-a-cresta-of-a-wave-down-memory-lane-to-the-retro-sweetshop/', 'snippet': \"Sep 4, 2010 ... Not as bad as Hershey bars which taste like acid reflux, or the 'chocolate flavoured coating' on the cheap choc ices from a van at a car boot\\xa0...\"}]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 47\n",
            "Wall time for execution: 0.75 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 143\n",
            "Wall time for execution: 1.95 seconds\n",
            "Warning: Invalid plan generated. Skipping worker and passing the question to solver.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 51\n",
            "Wall time for execution: 1.83 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt tokens: 0\n",
            "Completion tokens: 0\n",
            "Total tokens used in this chain: 76\n",
            "Wall time for execution: 1.60 seconds\n",
            "Warning: Invalid planner evidence format: #E1: According to Google, Stephen Colbert is a liberal and has been critical of Trump in the past.\n",
            "Warning: Invalid planner evidence format: #E2: According to Google, Stephen Colbert has not publicly disclosed his voting record.\n",
            "Warning: Invalid plan generated. Skipping worker and passing the question to solver.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO:\n",
        "\n",
        "[x] add baseline\n",
        "\n",
        "[] native google search retrieval\n",
        "\n",
        "[x] callback/count tokens for react\n",
        "\n",
        "[x] ReWOO\n",
        "\n",
        "[x] add walltime to all other functions -> # token, time, accuracy\n",
        "\n",
        "[] add everything to a class, run queries at concurrently\n",
        "\n",
        "[] deepseek"
      ],
      "metadata": {
        "id": "6AJANtdgltx6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNR-ttnzyPeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}